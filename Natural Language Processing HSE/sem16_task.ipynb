{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XkG_ZcvCuQJ0"
   },
   "source": [
    "# Майнор ВШЭ\n",
    "## Прикладные задачи анализа данных 2020\n",
    "## Обучение с подкреплением: CrossEntropy Method (CEM), Deep CEM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UM7vcYORuQJ1"
   },
   "source": [
    "## 1. Crossentropy Method\n",
    "\n",
    "В этой пункте мы посмотрим на то, как решить задачи RL с помощью метода crossentropy.\n",
    "\n",
    "Рассмотрим пример с задачей Taxi [Dietterich, 2000]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "i5yU3RZuuQJ2",
    "outputId": "95a3ba3f-505e-4a54-bc45-94264b75f87d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Xyi1DDjQuQJ8",
    "outputId": "22e8e06b-156a-4b9b-888f-fffd17009f3b"
   },
   "outputs": [],
   "source": [
    "n_states  = env.observation_space.n\n",
    "n_actions = env.action_space.n  \n",
    "\n",
    "print(f\"состояний: {n_states} действий: {n_actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hR-L-rHquQKA"
   },
   "source": [
    "В этот раз нашей стратегией будет вероятностной распределение. \n",
    "\n",
    "$\\pi(s,a) = P(a|s)$\n",
    "\n",
    "Для задачи такси мы можем использовать таблицу: \n",
    "\n",
    "policy[s,a] = P(выбрать действие a | в состоянии s)\n",
    "\n",
    "Создадим \"равномерную\" стратегию в виде двумерного массива с \n",
    "равномерным распределением по действиям и сгенерируем игровую сессию с такой стратегией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sVBlXo-huQKB"
   },
   "outputs": [],
   "source": [
    "def initialize_policy(n_states, n_actions):\n",
    "    ####### Здесь ваш код ##########\n",
    "    raise NotImplementedError\n",
    "    ################################\n",
    "    return policy\n",
    "\n",
    "policy = initialize_policy(n_states, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-xqVJHT4uQKE"
   },
   "outputs": [],
   "source": [
    "assert type(policy) in (np.ndarray, np.matrix)\n",
    "assert np.allclose(policy, 1./n_actions)\n",
    "assert np.allclose(np.sum(policy, axis=1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iDx_ek7vuQKH"
   },
   "source": [
    "### Генерация сессий взаимодейтсвия со средой.\n",
    "\n",
    "Мы будем запоминать все состояния, действия и вознаграждения за эпизод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QlsAeF7EuQKI"
   },
   "outputs": [],
   "source": [
    "def generate_session(env, policy, t_max=10**4):\n",
    "    \"\"\"\n",
    "    Игра идет до конца эпизода или до t_max шагов в окружении. \n",
    "    :param policy: [n_states,n_actions] \n",
    "    :returns: states - список состояний, actions - список действий, total_reward - итоговое вознаграждение\n",
    "    \"\"\"\n",
    "    states, actions = [], []\n",
    "    total_reward = 0.\n",
    "\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # Hint: you can use np.random.choice for sampling action\n",
    "        # https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html\n",
    "        # a = \n",
    "        ####### Здесь ваш код ########## \n",
    "        raise NotImplementedError \n",
    "        ################################\n",
    "\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # Record information we just got from the environment.\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HM2-QI3UuQKL"
   },
   "outputs": [],
   "source": [
    "s, a, r = generate_session(env, policy)\n",
    "assert type(s) == type(a) == list\n",
    "assert len(s) == len(a)\n",
    "assert type(r) in [float, np.float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "gWqyUSLJuQKP",
    "outputId": "277a831b-2c42-42b1-a8f4-516e3c508c7d"
   },
   "outputs": [],
   "source": [
    "# посмотрим на изначальное распределение вознаграждения\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sample_rewards = [generate_session(env, policy, t_max=1000)[-1] for _ in range(200)]\n",
    "\n",
    "plt.hist(sample_rewards, bins=20)\n",
    "plt.vlines([np.percentile(sample_rewards, 50)], [0], [100], label=\"50'th percentile\", color='green')\n",
    "plt.vlines([np.percentile(sample_rewards, 90)], [0], [100], label=\"90'th percentile\", color='red')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hUDOpKxHuQKS"
   },
   "source": [
    "### Реализация метода crossentropy  \n",
    "\n",
    "Наша задача - выделить лучшие действия и состояния, т.е. такие, при которых было лучшее вознаграждение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FGIweAi5uQKT"
   },
   "outputs": [],
   "source": [
    "def select_elites(states_batch, actions_batch, \n",
    "                  rewards_batch, percentile=50):\n",
    "    \"\"\"\n",
    "    Выбирает состояния и действия с заданным перцентилем (rewards >= percentile)\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i]\n",
    "    \n",
    "    :returns: elite_states, elite_actions - одномерные \n",
    "    списки состояния и действия, выбранных сессий\n",
    "    \"\"\"\n",
    "    # нужно найти порог вознаграждения по процентилю\n",
    "    # reward_threshold =\n",
    "    ####### Здесь ваш код ##########\n",
    "    raise NotImplementedError\n",
    "    ################################\n",
    "    \n",
    "    \n",
    "    # в соответствии с найденным порогом - отобрать \n",
    "    # подходящие состояния и действия\n",
    "    # elite_states = \n",
    "    # elite_actions = \n",
    "    ####### Здесь ваш код ##########\n",
    "    raise NotImplementedError\n",
    "    ################################\n",
    "    \n",
    "    return elite_states, elite_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Q34NZBHHuQKW",
    "outputId": "2e067944-6ca2-43fb-d5db-71a527602997"
   },
   "outputs": [],
   "source": [
    "states_batch = [\n",
    "    [1, 2, 3],     # game1\n",
    "    [4, 2, 0, 2],  # game2\n",
    "    [3, 1],        # game3\n",
    "]\n",
    "\n",
    "actions_batch = [\n",
    "    [0, 2, 4],     # game1\n",
    "    [3, 2, 0, 1],  # game2\n",
    "    [3, 3],        # game3\n",
    "]\n",
    "rewards_batch = [\n",
    "    3,  # game1\n",
    "    4,  # game2\n",
    "    5,  # game3\n",
    "]\n",
    "\n",
    "test_result_0 = select_elites(states_batch, actions_batch, rewards_batch, percentile=0)\n",
    "test_result_30 = select_elites(states_batch, actions_batch, rewards_batch, percentile=30)\n",
    "test_result_90 = select_elites(states_batch, actions_batch, rewards_batch, percentile=90)\n",
    "test_result_100 = select_elites(states_batch, actions_batch, rewards_batch, percentile=100)\n",
    "\n",
    "assert np.all(\n",
    "    test_result_0[0] == [1, 2, 3, 4, 2, 0, 2, 3, 1]) \\\n",
    "       and np.all(\n",
    "    test_result_0[1] == [0, 2, 4, 3, 2, 0, 1, 3, 3]), \\\n",
    "    \"Для процентиля 0 необходимо выбрать все состояния \" \\\n",
    "    \"и действия в хронологическом порядке\"\n",
    "\n",
    "assert np.all(test_result_30[0] == [4, 2, 0, 2, 3, 1])\\\n",
    "   and np.all(test_result_30[1] == [3, 2, 0, 1, 3, 3]), \\\n",
    "    \"Для процентиля 30 необходимо выбрать \" \\\n",
    "    \"состояния/действия из [3:]\"\n",
    "assert np.all(test_result_90[0] == [3, 1]) and \\\n",
    "       np.all(test_result_90[1] == [3, 3]), \\\n",
    "    \"Для процентиля 90 необходимо выбрать состояния \" \\\n",
    "    \"действия одной игры\"\n",
    "assert np.all(test_result_100[0] == [3, 1]) and \\\n",
    "       np.all(test_result_100[1] == [3, 3]), \\\n",
    "    \"Проверьте использование знаков: >=,  >. \" \\\n",
    "    \"Также проверьте расчет процентиля\"\n",
    "print(\"Тесты пройдены!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZZzLzP7PuQKb"
   },
   "source": [
    "Теперь мы хотим написать обновляющуюся стратегию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PecfT_xEuQKc"
   },
   "outputs": [],
   "source": [
    "def update_policy(elite_states,elite_actions):\n",
    "    \"\"\"\n",
    "    Новой стратегией будет:\n",
    "    policy[s_i,a_i] ~ #[вхождения  si/ai в лучшие states/actions]\n",
    "    \n",
    "    Не забудьте про нормализацию состояний.\n",
    "    Если какое-то состояние не было посещено, \n",
    "    то используйте равномерное распределение 1./n_actions\n",
    "    \n",
    "    :param elite_states:  список состояний\n",
    "    :param elite_actions: список действий\n",
    "    \"\"\"\n",
    "    new_policy = np.zeros([n_states,n_actions])\n",
    "    for state in range(n_states):\n",
    "        # обновляем стратегию - нормируем новые частоты \n",
    "        # действий и не забываем про непосещенные состояния\n",
    "        # new_policy[state, a] =         \n",
    "        ####### Здесь ваш код ##########\n",
    "        raise NotImplementedError\n",
    "        ################################\n",
    "        \n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DsCFXHMRuQKf",
    "outputId": "c9a057c5-70a2-4de1-e24b-6812e4ddd7e3"
   },
   "outputs": [],
   "source": [
    "elite_states, elite_actions = (\n",
    "    [1, 2, 3, 4, 2, 0, 2, 3, 1],\n",
    "    [0, 2, 4, 3, 2, 0, 1, 3, 3])\n",
    "\n",
    "new_policy = update_policy(elite_states, elite_actions)\n",
    "\n",
    "assert np.isfinite(\n",
    "    new_policy).all(), \"Стратегия не должна содержать \" \\\n",
    "                       \"NaNs или +-inf. Проверьте \" \\\n",
    "                       \"деление на ноль. \"\n",
    "assert np.all(\n",
    "    new_policy >= 0), \"Стратегия не должна содержать \" \\\n",
    "                      \"отрицательных вероятностей \"\n",
    "assert np.allclose(new_policy.sum(axis=-1),\n",
    "                   1), \"Суммарная\\ вероятность действий\"\\\n",
    "                       \"для состояния должна равняться 1\"\n",
    "reference_answer = np.array([\n",
    "    [1., 0., 0., 0., 0.],\n",
    "    [0.5, 0., 0., 0.5, 0.],\n",
    "    [0., 0.33333333, 0.66666667, 0., 0.],\n",
    "    [0., 0., 0., 0.5, 0.5]])\n",
    "assert np.allclose(new_policy[:4, :5], reference_answer)\n",
    "print(\"Тесты пройдены!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VYRLBwtvuQKi"
   },
   "source": [
    "### Цикл обучения\n",
    "\n",
    "Визуализириуем наш процесс обучения и также будем измерять распределение получаемых за сессию вознаграждений "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tmBnonZCuQKj"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def show_progress(rewards_batch, log, percentile, reward_range=[-990, +10]):\n",
    "    \"\"\"\n",
    "    Удобная функция, для визуализации результатов.\n",
    "    \"\"\"\n",
    "\n",
    "    mean_reward = np.mean(rewards_batch)\n",
    "    threshold = np.percentile(rewards_batch, percentile)\n",
    "    log.append([mean_reward, threshold])\n",
    "    \n",
    "    plt.figure(figsize=[8, 4])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(list(zip(*log))[0], label='Mean rewards')\n",
    "    plt.plot(list(zip(*log))[1], label='Reward thresholds')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(rewards_batch, range=reward_range)\n",
    "    plt.vlines([np.percentile(rewards_batch, percentile)],\n",
    "               [0], [100], label=\"percentile\", color='red')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    clear_output(True)\n",
    "    print(\"mean reward = %.3f, threshold=%.3f\" % (mean_reward, threshold))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GXhGLvjPuQKm"
   },
   "outputs": [],
   "source": [
    "# инициализируем стратегию\n",
    "policy = initialize_policy(n_states, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "vYncUOBfuQKo",
    "outputId": "2701be64-f48f-4b1c-80e9-6e194d9ad318"
   },
   "outputs": [],
   "source": [
    "n_sessions = 250  # количество сессий для сэмплирования\n",
    "percentile = 50  # перцентиль \n",
    "learning_rate = 0.5 # то как быстро стратегия будет обновляться \n",
    "\n",
    "log = []\n",
    "\n",
    "for i in range(100):\n",
    "    # генерируем n_sessions сессий\n",
    "    # %time sessions = []\n",
    "    ####### Здесь ваш код ##########\n",
    "    raise NotImplementedError\n",
    "    ################################\n",
    "    \n",
    "    states_batch,actions_batch,rewards_batch = zip(*sessions)\n",
    "    # отбираем лучшие действия и состояния ###\n",
    "    # elite_states, elite_actions = \n",
    "    ####### Здесь ваш код ##########\n",
    "    raise NotImplementedError\n",
    "    ################################\n",
    "    \n",
    "    # обновляем стратегию\n",
    "    # new_policy =\n",
    "    ####### Здесь ваш код ##########\n",
    "    raise NotImplementedError\n",
    "    ################################\n",
    "    \n",
    "    policy = learning_rate * new_policy + (1 - learning_rate) * policy\n",
    "\n",
    "    # display results on chart\n",
    "    show_progress(rewards_batch, log, percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3xIsESNpuQKr"
   },
   "source": [
    "### Посмотрим на результаты\n",
    "Задача такси быстро сходится, начиня с вознаграждения -1000 к почти оптимальному значению, а потом опять падает до -50/-100. Это вызвано случайностью в самом окружении - случайное начальное состояние пассажира и такси, в начале каждого эпизода. \n",
    "\n",
    "В случае если алгоритм CEM не сможет научиться тому, как решить задачу из какого-то стартового положения, он просто отбросит этот эпизод, т.к. не будет сессий, которые переведут этот эпизод в топ лучших. \n",
    "\n",
    "Для решения этой проблемы можно уменьшить threshold (порог лучших состояний) или изменить способ оценки стратегии, используя новую стратегию, полученную из каждого начального состояния и действия (теоретически правильный способ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_-nMGpEfuQKs"
   },
   "source": [
    "## 2. Deep CEM\n",
    "\n",
    "В данной части мы рассмотрим применение CEM вместе с нейронной сетью.\n",
    "Будем обучать многослойную нейронную сеть для решения простой задачи с непрерывным пространством действий.\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/8f39c7f54a7798e7f80c9ec5c0bb610696e5c5b7/68747470733a2f2f7469702e64756b652e6564752f696e646570656e64656e745f6c6561726e696e672f677265656b2f6c6573736f6e2f64696767696e675f6465657065725f66696e616c2e6a7067\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sWFuA1hWuQKs"
   },
   "source": [
    "Будем тестировать нашего нового агента на известной задаче перевернутого маятника с непрерывным пространством действий.\n",
    "https://gym.openai.com/envs/CartPole-v0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-ZhZahUGuQKt",
    "outputId": "4a1aa2a5-cbca-4b4b-f82f-9ec65e2acd96"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]\n",
    "                                        \n",
    "print(f\"состояний: {n_states} действий: {n_actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UgRpAtU-uQKw"
   },
   "source": [
    "### Стратегия с нейронной сетью\n",
    "\n",
    "Попробуем заменить метод обновления вероятностей на нейронную сеть. \n",
    "Будем пользоваться упрощенной реализацией нейронной сети из пакета Scikit-learn.\n",
    "Нам потребуется: \n",
    "* agent.partial_fit(states, actions) - делает один проход обучения по данным. Максимизирует вероятность :actions: из :states:\n",
    "* agent.predict_proba(states) - предсказыает вероятность каждого из действий, в виде матрицы размера [len(states), n_actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "5fOPMKcYuQKx",
    "outputId": "d8019638-3a51-4735-9f28-b2f673ead3d6"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "agent = MLPClassifier(\n",
    "    hidden_layer_sizes=(20, 20),\n",
    "    activation='tanh',\n",
    ")\n",
    "\n",
    "# initialize agent to the dimension of state space and number of actions\n",
    "agent.partial_fit([env.reset()] * n_actions, range(n_actions), range(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R9W-5nV3uQK0"
   },
   "outputs": [],
   "source": [
    "def generate_session(env, agent, t_max=1000):\n",
    "    \n",
    "    states,actions = [],[]\n",
    "    total_reward = 0\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        # предсказываем вероятности действий по сети и \n",
    "        # выбираем одно действие\n",
    "        # probs = \n",
    "        # a = \n",
    "        ####### Здесь ваш код ##########\n",
    "        raise NotImplementedError\n",
    "        ################################\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "    return states,actions,total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "0pntOSnguQK2",
    "outputId": "be57e836-f5ae-4f31-e9f1-578dad7813fb"
   },
   "outputs": [],
   "source": [
    "dummy_states, dummy_actions, dummy_reward = generate_session(env, agent, t_max=5)\n",
    "print(\"состояния:\", np.stack(dummy_states))\n",
    "print(\"действия:\", dummy_actions)\n",
    "print(\"вознаграждение:\", dummy_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "colab_type": "code",
    "id": "HVDwmDFAuQK4",
    "outputId": "ebf837f8-baea-4e04-a169-905c2585ad1c"
   },
   "outputs": [],
   "source": [
    "n_sessions = 100\n",
    "percentile = 70\n",
    "log = []\n",
    "\n",
    "for i in range(100):\n",
    "    # генерируем n_sessions сессий\n",
    "    # sessions = []\n",
    "    ####### Здесь ваш код ##########\n",
    "    raise NotImplementedError\n",
    "    ################################\n",
    "    states_batch, actions_batch, rewards_batch = map(np.array, zip(*sessions))\n",
    "    \n",
    "    # отбираем лучшие действия и состояния \n",
    "    # elite_states, elite_actions = \n",
    "    ####### Здесь ваш код ##########\n",
    "    raise NotImplementedError\n",
    "    ################################\n",
    "    \n",
    "    # обновляем стратегию, для предсказания лучших состояний\n",
    "    # elite_actions(y) из elite_states(X)\n",
    "    ####### Здесь ваш код ##########\n",
    "    raise NotImplementedError\n",
    "    ################################\n",
    "    \n",
    "    show_progress(rewards_batch, log, percentile, reward_range=[0, np.max(rewards_batch)])\n",
    "\n",
    "    if np.mean(rewards_batch) > 190:\n",
    "        print(\"Принято!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Sem16full.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
